{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Steps are:\n",
    "\n",
    "- select a reconstruction time\n",
    "- the code determines which paleogeography stage this falls within, gets the start and end times\n",
    "- load the relevant precomputed multipoint files, and in the process assign an integer to the different types for use in interpolation steps (e.g. set land to be 1, shallow marine to be 2, etc)\n",
    "\n",
    "- for land and marine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[   6.   14.   22.   33.   45.   53.   76.   90.  105.  126.  140.  152.\n",
      "  169.  195.  218.  232.  255.  277.  287.  302.  328.  348.  368.  396.]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../paleogeography')\n",
    "sys.path.append('../paleotopography')\n",
    "sys.path.append('../pigplates/')\n",
    "sys.path.append('../pygplates2014/')\n",
    "sys.path.append('/Users/Simon/GIT/PlateTectonicTools/')\n",
    "\n",
    "import pygplates\n",
    "import glob, re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import xarray as xr\n",
    "\n",
    "import polygon_processing as pp\n",
    "import paleogeography as pg\n",
    "import paleogeography_tweening as pgt\n",
    "import paleotopography as pt\n",
    "\n",
    "from proximity_query import *\n",
    "from create_gpml import create_gpml_regular_long_lat_mesh\n",
    "import points_in_polygons\n",
    "from sphere_tools import sampleOnSphere\n",
    "import points_spatial_tree\n",
    "\n",
    "from ptt.utils.call_system_command import call_system_command\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "reconstruction_basedir = '../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles/'\n",
    "tween_basedir = './tween_feature_collections_gpml/'\n",
    "file_format = 'gpml'\n",
    "\n",
    "output_dir = './paleotopo_grids_netcdf4/'\n",
    "\n",
    "netcdf3_output = False\n",
    "\n",
    "#rotation_model = pygplates.RotationModel(['%s/Global_EB_250-0Ma_GK07_Matthews++.rot' % reconstruction_basedir,\n",
    "#                                          '%s/Global_EB_410-250Ma_GK07_Matthews++.rot' % reconstruction_basedir])\n",
    "rotation_file = '%s/Global_EarthByte_230-0Ma_GK07_AREPS.rot' % reconstruction_basedir\n",
    "\n",
    "\n",
    "COBterrane_file = '%s/Global_EarthByte_GeeK07_COB_Terranes_Matthews_etal.gpml' % reconstruction_basedir\n",
    "\n",
    "agegrid_file_template = '/Users/Simon/Data/AgeGrids/Agegrids_30m_20151002_2015_v1_r756/agegrid_30m_%d.grd'\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Set the heights for different environment\n",
    "#############################################\n",
    "depth_for_unknown_ocean = -1000\n",
    "# ----------------------------------\n",
    "shallow_marine_elevation = -200.\n",
    "# ----------------------------------\n",
    "lowland_elevation = 200.\n",
    "# ----------------------------------\n",
    "max_mountain_elevation = 1500.\n",
    "# NOTE - this height is actually the mountain height IN ADDITION TO the lowland height\n",
    "# so that the maximum absolute elevation would be [lowland_elevation + max_mountain_elevation]\n",
    "# TODO should call this 'mountain_relief'???\n",
    "#############################################\n",
    "\n",
    "# the grid sampling for the output\n",
    "sampling = 1.0\n",
    "\n",
    "# this number controls how small polygons are exclude when merging the COB terranes into \n",
    "# land/sea masking polygons\n",
    "area_threshold = 0.0001\n",
    "\n",
    "# used for quadtree\n",
    "subdivision_depth = 2\n",
    "\n",
    "# this buffer defines the smoothness of the topography at the transition from 'lowland' to 'mountain'\n",
    "# the distance defined here is the distance over which heights ramp from the lowland elevation to the \n",
    "# mountain elevation defined above. (the ramping takes place from the edge of the mountain range inwards\n",
    "# towards the mountain interior). Any parts of the mountain range greater than this buffer distance from \n",
    "# the edge will have a uniform height equal to max_mountain_elevation\n",
    "mountain_buffer_distance_degrees = 1.\n",
    "#mountain_buffer_distance_degrees = 2.\n",
    "\n",
    "# choose here either 'ocean' or 'land'\n",
    "# this determines which grid takes precedence where both the age grid and the \n",
    "# paleogeographies overlap and contain valid values\n",
    "land_or_ocean_precedence = 'land'\n",
    "\n",
    "# this number is used in the final grdfilter step to smooth the output \n",
    "grid_smoothing_wavelength_kms = 400.\n",
    "\n",
    "time_min = 0.\n",
    "time_max = 230.\n",
    "time_step = 1.\n",
    "\n",
    "merge_with_bathymetry = True\n",
    "\n",
    "\n",
    "####################################################\n",
    "\n",
    "# make a sorted list of the (midpoint) times for paleogeography polygons\n",
    "tmp = glob.glob('%s/*/' % reconstruction_basedir)\n",
    "\n",
    "paleogeography_timeslice_list = []\n",
    "for tm in tmp:\n",
    "    paleogeography_timeslice_list.append(float(re.findall(r'\\d+Ma+',tm)[1][:-2]))\n",
    "\n",
    "paleogeography_timeslice_list.sort()\n",
    "\n",
    "paleogeography_timeslice_list = np.array(paleogeography_timeslice_list)\n",
    "\n",
    "print paleogeography_timeslice_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Time 0.00Ma\n",
      "\n",
      "Selected Time is in the stage 0.00Ma to 6.00Ma\n",
      "Temporary fix for valid time\n",
      "./present_day_paleogeography.gmt\n",
      "Working on Time 1.00Ma\n",
      "\n",
      "Selected Time is in the stage 0.00Ma to 6.00Ma\n",
      "./present_day_paleogeography.gmt\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_6Ma/m_fig64_11_2_PresentDay_Paleogeog_Matthews2016_6.00Ma.shp']\n",
      "Working on Time 2.00Ma\n",
      "\n",
      "Selected Time is in the stage 0.00Ma to 6.00Ma\n",
      "./present_day_paleogeography.gmt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f7e7cbfa1cf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m                         \u001b[0mlowland_elevation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshallow_marine_elevation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_mountain_elevation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_for_unknown_ocean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                         \u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmountain_buffer_distance_degrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marea_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                         grid_smoothing_wavelength_kms, merge_with_bathymetry, subdivision_depth)\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f7e7cbfa1cf3>\u001b[0m in \u001b[0;36mpaleotopography_job\u001b[0;34m(reconstruction_time, paleogeography_timeslice_list, tween_basedir, reconstruction_basedir, output_dir, file_format, rotation_file, COBterrane_file, lowland_elevation, shallow_marine_elevation, max_mountain_elevation, depth_for_unknown_ocean, sampling, mountain_buffer_distance_degrees, area_threshold, grid_smoothing_wavelength_kms, merge_with_bathymetry, subdivision_depth)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# using the pg polygons that they should exactly correspond to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         dist_t1 = pt.get_distance_to_mountain_edge(mountains_t1_point_array,reconstruction_basedir,\n\u001b[0;32m--> 121\u001b[0;31m                                                    rotation_model,time_stage_min,area_threshold)\n\u001b[0m\u001b[1;32m    122\u001b[0m         dist_t2 = pt.get_distance_to_mountain_edge(mountains_t2_point_array,reconstruction_basedir,\n\u001b[1;32m    123\u001b[0m                                                    rotation_model,time_stage_max,area_threshold)\n",
      "\u001b[0;32m/Users/Simon/GIT/paleotopography/paleotopography.py\u001b[0m in \u001b[0;36mget_distance_to_mountain_edge\u001b[0;34m(point_array, reconstruction_basedir, rotation_model, time, area_threshold)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mpg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_paleogeography\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpg_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_polygons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpg_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrotation_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0msieve_polygons_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolygon_area_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marea_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/GIT/paleogeography/polygon_processing.pyc\u001b[0m in \u001b[0;36mmerge_polygons\u001b[0;34m(polygons, rotation_model, time, sampling, area_threshold, filename, return_raster)\u001b[0m\n\u001b[1;32m     12\u001b[0m                    return_raster=False):\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmultipoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_gpml_regular_long_lat_mesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mgrid_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m360\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/GIT/pigplates/create_gpml.pyc\u001b[0m in \u001b[0;36mcreate_gpml_regular_long_lat_mesh\u001b[0;34m(Sampling, filename, feature_type)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# call the function to create a multipoint feature, with user-defined type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0moutput_feature_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_gpml_velocity_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongitude_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatitude_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/GIT/pigplates/create_gpml.pyc\u001b[0m in \u001b[0;36mcreate_gpml_velocity_feature\u001b[0;34m(longitude_array, latitude_array, filename, feature_type)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Long and Lat are assumed to be 1d arrays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmulti_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygplates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiPointOnSphere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatitude_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlongitude_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Create a feature containing the multipoint feature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prepend 0 to the list, since this is not covered in published paleogeography sequence\n",
    "paleogeography_timeslice_list = np.hstack((0,paleogeography_timeslice_list))\n",
    "\n",
    "\n",
    "def paleotopography_job(reconstruction_time, paleogeography_timeslice_list, \n",
    "                        tween_basedir, reconstruction_basedir, output_dir, \n",
    "                        file_format, rotation_file, COBterrane_file, \n",
    "                        lowland_elevation, shallow_marine_elevation, max_mountain_elevation, depth_for_unknown_ocean, \n",
    "                        sampling, mountain_buffer_distance_degrees, area_threshold,\n",
    "                        grid_smoothing_wavelength_kms, merge_with_bathymetry, subdivision_depth):\n",
    "\n",
    "\n",
    "    print 'Working on Time %0.2fMa\\n' % reconstruction_time \n",
    "        \n",
    "    rotation_model = pygplates.RotationModel(rotation_file)\n",
    "                        \n",
    "    # find times that bracket the selected exact time in the paleogeography source files\n",
    "    time_stage_max = paleogeography_timeslice_list[np.where(paleogeography_timeslice_list>reconstruction_time)[0][0]]\n",
    "    time_stage_min = paleogeography_timeslice_list[np.where(paleogeography_timeslice_list<=reconstruction_time)[0][-1]]\n",
    "\n",
    "    # Note the logic for selecting the times:\n",
    "    # The main issue is that each set of paleogeography polygons has a defined 'midpoint' time\n",
    "    # --> if the reconstruction time is between these, the choice of t1 and t2 is obvious\n",
    "    # --> if the reconstruction time matches one of these times, then we can work directly on\n",
    "    #     the geometries that match this time - hence the two routes through the if statement below\n",
    "    \n",
    "    print 'Selected Time is in the stage %0.2fMa to %0.2fMa' % (time_stage_min,time_stage_max)\n",
    "\n",
    "    land_points_file = '%s/tweentest_land_%0.2fMa_%0.2fMa.%s' % (tween_basedir,time_stage_min,time_stage_max,file_format)\n",
    "    marine_points_file = '%s/tweentest_ocean_%0.2fMa_%0.2fMa.%s' % (tween_basedir,time_stage_min,time_stage_max,file_format)\n",
    "    mountains_going_up_file = '%s/mountain_transgression_%0.2fMa_%0.2fMa.%s' % (tween_basedir,time_stage_min,time_stage_max,file_format)\n",
    "    mountains_going_down_file = '%s/mountain_regression_%0.2fMa_%0.2fMa.%s' % (tween_basedir,time_stage_min,time_stage_max,file_format)\n",
    "    mountains_stable_file = '%s/mountain_stable_%0.2fMa_%0.2fMa.%s' % (tween_basedir,time_stage_min,time_stage_max,file_format)\n",
    "\n",
    "    \n",
    "    # get a nx3 array defining the points above sea-level, reconstructed to time of interest\n",
    "    # columns are [lat, long, elevation assigned for lowland]\n",
    "    land_point_array = pt.add_reconstructed_points_to_xyz(land_points_file,\n",
    "                                                          rotation_model,\n",
    "                                                          reconstruction_time,\n",
    "                                                          lowland_elevation)\n",
    "    \n",
    "    # get a nx3 array defining shallow marine areas, reconstructed to time of interest\n",
    "    # columns are [lat, long, elevation assigned for shallow marine]\n",
    "    marine_point_array = pt.add_reconstructed_points_to_xyz(marine_points_file,\n",
    "                                                            rotation_model,\n",
    "                                                            reconstruction_time,\n",
    "                                                            shallow_marine_elevation)\n",
    "    \n",
    "    # Note that the two arrays just created are based on 'regular' lat/long grids, but \n",
    "    # are not aligned with the regular lat/long grid that we want to output\n",
    "    # since they are (usually) reconstructed to a different time from the one at which they\n",
    "    # were created (and anyway may be at a different resolution to the grid sampling specified\n",
    "    # here)\n",
    "\n",
    "    # combine the previous two arrays\n",
    "    pg_point_array = np.vstack((land_point_array,marine_point_array))\n",
    "\n",
    "    # get a merged version of COB terranes, optionally excluding polygons that are small in area\n",
    "    # TODO deal with donut polygons better\n",
    "    sieve_polygons = pt.get_merged_cob_terrane_polygons(COBterrane_file,rotation_model,\n",
    "                                                        reconstruction_time,sampling)\n",
    "\n",
    "    # get arrays defining the land and sea based on which points fall within the COB terranes\n",
    "    # NOTE this step is where we create the points that ARE on the regular lat/long grid we \n",
    "    # will ultimately output\n",
    "    (lat,lon,zval,\n",
    "     lat_deep,lon_deep,zval_deep) = pt.get_land_sea_multipoints(sieve_polygons,sampling,depth_for_unknown_ocean,\n",
    "                                                                subdivision_depth=subdivision_depth)\n",
    "\n",
    "\n",
    "    # sample the land/marine points onto the points within the COB Terranes\n",
    "    # This will fill the gaps that exist within continents, and average out overlaps\n",
    "    d,l = sampleOnSphere(pg_point_array[:,0],pg_point_array[:,1],pg_point_array[:,2],\n",
    "                         np.array(lat),np.array(lon),n=1)\n",
    "\n",
    "    land_marine_interp_points = pg_point_array[:,2].ravel()[l]\n",
    "\n",
    "    # At this point, the land points are all considered to be 'lowland'......\n",
    "    \n",
    "    ####################################\n",
    "    # Deal with the mountains\n",
    "    if np.equal(reconstruction_time,time_stage_min):\n",
    "        print 'Temporary fix for valid time'\n",
    "        #dat3 = add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,reconstruction_time,3)\n",
    "        dat4 = pt.add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,reconstruction_time+0.01,1)\n",
    "        dat5 = pt.add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,reconstruction_time+0.01,1)\n",
    "        mountains_tr_point_array = np.vstack((dat4,dat5))\n",
    "        \n",
    "        dist_tr = pt.get_distance_to_mountain_edge(mountains_tr_point_array,reconstruction_basedir,\n",
    "                                                   rotation_model,reconstruction_time,area_threshold)\n",
    "        dist_tr_cap = np.array(dist_tr)\n",
    "        dist_tr_cap[np.array(dist_tr)>mountain_buffer_distance_degrees] = mountain_buffer_distance_degrees\n",
    "\n",
    "        dist_tr_cap = dist_tr_cap*mountains_tr_point_array[:,2]\n",
    "\n",
    "        normalized_mountain_elevation = dist_tr_cap\n",
    "        \n",
    "    else:\n",
    "        # load in the mountain points but at three different times: t1 and t2, and the reconstruction time\n",
    "        # note that these three arrays should all be identical in size, since they are the same multipoints\n",
    "        # just reconstructed to three slightly different times\n",
    "        dat3 = pt.add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,time_stage_max,1)\n",
    "        dat4 = pt.add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,time_stage_max,1)\n",
    "        dat5 = pt.add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,time_stage_max,1)\n",
    "        mountains_t2_point_array = np.vstack((dat3,dat4,dat5))\n",
    "\n",
    "        dat3 = pt.add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,time_stage_min+0.01,1)\n",
    "        dat4 = pt.add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,time_stage_min+0.01,1)\n",
    "        dat5 = pt.add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,time_stage_min+0.01,1)\n",
    "        mountains_t1_point_array = np.vstack((dat3,dat4,dat5))\n",
    "\n",
    "        dat3 = pt.add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,reconstruction_time,1)\n",
    "        dat4 = pt.add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,reconstruction_time,1)\n",
    "        dat5 = pt.add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,reconstruction_time,1)\n",
    "        mountains_tr_point_array = np.vstack((dat3,dat4,dat5))\n",
    "\n",
    "        # calculate distances of the mountain points to the edge of the mountain region at t1 and t2,\n",
    "        # using the pg polygons that they should exactly correspond to \n",
    "        dist_t1 = pt.get_distance_to_mountain_edge(mountains_t1_point_array,reconstruction_basedir,\n",
    "                                                   rotation_model,time_stage_min,area_threshold)\n",
    "        dist_t2 = pt.get_distance_to_mountain_edge(mountains_t2_point_array,reconstruction_basedir,\n",
    "                                                   rotation_model,time_stage_max,area_threshold)\n",
    "        \n",
    "        #is_in_orogeny_index = find_mountain_type(mountains_tr_point_array,\n",
    "        #                                         orogeny_feature_filename,\n",
    "        #                                         reconstruction_time)\n",
    "\n",
    "\n",
    "        # cap the distances at some arbitrary value defined earlier\n",
    "        dist_t1_cap = np.array(dist_t1)\n",
    "        dist_t1_cap[np.array(dist_t1)>mountain_buffer_distance_degrees] = mountain_buffer_distance_degrees\n",
    "\n",
    "        dist_t1_cap = dist_t1_cap*mountains_t1_point_array[:,2]\n",
    "        \n",
    "        dist_t2_cap = np.array(dist_t2)\n",
    "        dist_t2_cap[np.array(dist_t2)>mountain_buffer_distance_degrees] = mountain_buffer_distance_degrees\n",
    "        \n",
    "        dist_t2_cap = dist_t2_cap*mountains_t2_point_array[:,2]\n",
    "\n",
    "        # get the normalised time within this time stage\n",
    "        # for example we are at 0.25 between the t1 and t2\n",
    "        t_diff = (time_stage_max-time_stage_min)\n",
    "        t_norm = (reconstruction_time-time_stage_min)/t_diff\n",
    "        #print t_diff, t_norm\n",
    "\n",
    "        # use 1d interpolation to get the 'normalized' height of the mountains at the preceding\n",
    "        # and subsequent times to the specific reconstruction time\n",
    "        # [note this is not spatial interpolation - rather it is interpolation at each individual point\n",
    "        # between the heights at earlier and later times]\n",
    "        tmp = np.vstack((dist_t1_cap,dist_t2_cap))\n",
    "        f = interpolate.interp1d([0,1],tmp.T)\n",
    "        normalized_mountain_elevation = f(t_norm)\n",
    "    \n",
    "    \n",
    "    # interpolate the elevations at tr onto the regular long lat points that we will ultimately use \n",
    "    # for the grid output\n",
    "    # note the k value here controls number of neighbouring points used in inverse distance average\n",
    "    d,l = sampleOnSphere(mountains_tr_point_array[:,0],mountains_tr_point_array[:,1],normalized_mountain_elevation,\n",
    "                         np.array(lat),np.array(lon),k=4)\n",
    "    w = 1./d**2\n",
    "    normalized_mountain_elevation_interp_points = np.sum(w * normalized_mountain_elevation.ravel()[l],axis=1) / np.sum(w,axis=1)\n",
    "\n",
    "    # this index isolates only those points that are within a certain distance of the mountain range\n",
    "    # (since the interpolation will give values everywhere in the 'land', so we want to re-isolate only \n",
    "    # those points that fall within the 'mountain' regions)\n",
    "    # TODO this should be set to the sampling??\n",
    "    mountain_proximity_index = np.degrees(np.min(d,axis=1))< sampling*2 #mountain_buffer_distance_degrees\n",
    "\n",
    "    \n",
    "    #####################################\n",
    "    # Put the grid together\n",
    "    #####################################\n",
    "    \n",
    "    # write the land/marine points to a file\n",
    "    land_marine_xyz_file = tempfile.NamedTemporaryFile()\n",
    "    mountain_xyz_file = tempfile.NamedTemporaryFile()\n",
    "    land_marine_nc_file = tempfile.NamedTemporaryFile()\n",
    "    mountain_nc_file = tempfile.NamedTemporaryFile()\n",
    "    \n",
    "    pt.write_xyz_file(land_marine_xyz_file.name,zip(lon+lon_deep,\n",
    "                                                lat+lat_deep,\n",
    "                                                np.hstack((land_marine_interp_points,zval_deep))))\n",
    "\n",
    "    # convert the normalized mountain elevations to metres, then write to file\n",
    "    mountain_elevation_factor = max_mountain_elevation/mountain_buffer_distance_degrees\n",
    "    mountain_elevation_array = normalized_mountain_elevation_interp_points[mountain_proximity_index]*mountain_elevation_factor\n",
    "    pt.write_xyz_file(mountain_xyz_file.name,zip(np.array(lon)[mountain_proximity_index],\n",
    "                                             np.array(lat)[mountain_proximity_index],\n",
    "                                             mountain_elevation_array))\n",
    "\n",
    "    # all the points are already on the same regular lat/long grid (but with gaps) - just \n",
    "    # need to piece them all together and combine.\n",
    "    # Note we assume the the mountain elevation is the height IN ADDITION to the lowland elevation\n",
    "    # so that we can simply add them\n",
    "    call_system_command(['gmt', 'xyz2grd', land_marine_xyz_file.name, '-Rd', '-I%0.8f' % sampling, \n",
    "                         '-G%s' % land_marine_nc_file.name])\n",
    "    call_system_command(['gmt', 'xyz2grd', mountain_xyz_file.name, '-Rd', '-I%0.8f' % sampling, '-di0', \n",
    "                         '-G%s' % mountain_nc_file.name])\n",
    "    call_system_command(['gmt', 'grdmath', mountain_nc_file.name, land_marine_nc_file.name, 'ADD', '=', \n",
    "                         '%s/paleotopo_%0.2fMa.nc' % (output_dir,reconstruction_time)])\n",
    "\n",
    "    # clean-up temp files\n",
    "    land_marine_xyz_file.delete\n",
    "    mountain_xyz_file.delete\n",
    "    land_marine_nc_file.delete\n",
    "    mountain_nc_file.delete\n",
    "    \n",
    "    # load result back into python\n",
    "    topoX,topoY,topoZ = pg.load_netcdf('%s/paleotopo_%0.2fMa.nc' % (output_dir,reconstruction_time))\n",
    "\n",
    "    \n",
    "    if merge_with_bathymetry:\n",
    "    \n",
    "        # PALEOBATHYMETRY based on age grids\n",
    "        # load age grid for this time and calculate paleobathymetry\n",
    "        agegrid_file = agegrid_file_template % reconstruction_time\n",
    "\n",
    "        ageX,ageY,ageZ = pg.load_netcdf(agegrid_file)\n",
    "\n",
    "        paleodepth = pg.age2depth(ageZ,model='GDH1')\n",
    "\n",
    "        # get index for grid nodes where age grid is nan, replace values with topography/shallow bathymetry\n",
    "        not_bathy_index = np.isnan(paleodepth)\n",
    "        paleodepth[not_bathy_index] = topoZ[not_bathy_index]\n",
    "\n",
    "        paleotopobathy_nc_file = tempfile.NamedTemporaryFile()\n",
    "        paleotopobathy_smooth_nc_file = tempfile.NamedTemporaryFile()\n",
    "        \n",
    "        # save the merged grid (forcing compatibility with GPlates-readable netCDF in case it helps)\n",
    "        ds = xr.DataArray(paleodepth,\n",
    "                          coords=[('lat',topoY),('lon',topoX)],\n",
    "                          name='elevation')\n",
    "        ds.to_netcdf(paleotopobathy_nc_file.name,format='NETCDF3_CLASSIC')\n",
    "\n",
    "        # smooth the grid using GMT [wavelength is optional\n",
    "        #pg.smooth_topography_grid('paleotopobathy.nc','paleotopobathy_smooth_%0.2fMa.nc' % reconstruction_time,400.)\n",
    "        call_system_command(['gmt', 'grdfilter', paleotopobathy_nc_file.name, '-G%s' % paleotopobathy_smooth_nc_file.name, \n",
    "                             '-Fg%0.2f' % grid_smoothing_wavelength_kms, '-fg', '-D4', '-Vl'])\n",
    "\n",
    "        if netcdf3_output:\n",
    "            # finally, once again force GPlates-readable netCDF (ie netCDF v3) and put the \n",
    "            # grid in the output folder with a filename containing the age\n",
    "            call_system_command(['gmt', 'grdconvert', paleotopobathy_smooth_nc_file.name, \n",
    "                                 '-G%s=cf' % '%s/paleotopobathy_smooth_%0.2fMa.nc' % (output_dir,reconstruction_time)])\n",
    "        else:\n",
    "            call_system_command(['cp', paleotopobathy_smooth_nc_file.name, \n",
    "                                '%s/paleotopobathy_smooth_%0.2fMa.nc' % (output_dir,reconstruction_time)])\n",
    "        \n",
    "        # load and plot the result\n",
    "        topo_smoothX,topo_smoothY,topo_smoothZ = pg.load_netcdf(paleotopobathy_smooth_nc_file.name)\n",
    "        #\n",
    "        plt.figure(figsize=(25,11))\n",
    "        plt.imshow(topo_smoothZ,origin='lower',\n",
    "                   extent=[-180,180,-90,90],\n",
    "                   cmap=plt.cm.terrain,vmin=-5000,vmax=5000)\n",
    "        plt.title('%0.2fMa' % reconstruction_time)\n",
    "        plt.colorbar()\n",
    "        plt.savefig('%s/paleotopobathy_smooth_%0.2fMa.png' % (output_dir,reconstruction_time))\n",
    "        plt.close()\n",
    "\n",
    "        paleotopobathy_nc_file.delete\n",
    "        paleotopobathy_smooth_nc_file.delete\n",
    "        \n",
    "\n",
    "for reconstruction_time in np.arange(time_min,time_max+time_step,time_step):\n",
    "\n",
    "    paleotopography_job(reconstruction_time, paleogeography_timeslice_list, \n",
    "                        tween_basedir, reconstruction_basedir, output_dir, \n",
    "                        file_format, rotation_file, COBterrane_file, \n",
    "                        lowland_elevation, shallow_marine_elevation, max_mountain_elevation, depth_for_unknown_ocean, \n",
    "                        sampling, mountain_buffer_distance_degrees, area_threshold,\n",
    "                        grid_smoothing_wavelength_kms, merge_with_bathymetry, subdivision_depth)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
