{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Steps are:\n",
    "\n",
    "- select a reconstruction time\n",
    "- the code determines which paleogeography stage this falls within, gets the start and end times\n",
    "- load the relevant precomputed multipoint files, and in the process assign an integer to the different types for use in interpolation steps (e.g. set land to be 1, shallow marine to be 2, etc)\n",
    "\n",
    "- for land and marine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pygplates\n",
    "import glob, re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import xarray as xr\n",
    "\n",
    "import polygon_processing as pp\n",
    "import paleogeography as pg\n",
    "import paleogeography_tweening as pgt\n",
    "\n",
    "from proximity_query import *\n",
    "from create_gpml import create_gpml_regular_long_lat_mesh\n",
    "import points_in_polygons\n",
    "from sphere_tools import sampleOnSphere\n",
    "import points_spatial_tree\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "reconstruction_basedir = '../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles/'\n",
    "tween_basedir = './tween_feature_collections/'\n",
    "\n",
    "output_dir = './paleotopo_grids/'\n",
    "\n",
    "\n",
    "#rotation_model = pygplates.RotationModel(['%s/Global_EB_250-0Ma_GK07_Matthews++.rot' % reconstruction_basedir,\n",
    "#                                          '%s/Global_EB_410-250Ma_GK07_Matthews++.rot' % reconstruction_basedir])\n",
    "rotation_model = pygplates.RotationModel('%s/Global_EarthByte_230-0Ma_GK07_AREPS.rot' % reconstruction_basedir)\n",
    "\n",
    "\n",
    "COBterrane_file = '%s/Global_EarthByte_GeeK07_COB_Terranes_Matthews_etal.gpml' % reconstruction_basedir\n",
    "\n",
    "agegrid_file_template = '/Users/Simon/Data/AgeGrids/Agegrids_30m_20151002_2015_v1_r756/agegrid_30m_%d.grd'\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Set the heights for different environment\n",
    "#############################################\n",
    "depth_for_unknown_ocean = -1000\n",
    "# ----------------------------------\n",
    "shallow_marine_elevation = -200.\n",
    "# ----------------------------------\n",
    "lowland_elevation = 200.\n",
    "# ----------------------------------\n",
    "max_mountain_elevation = 1500.\n",
    "# NOTE - this height is actually the mountain height IN ADDITION TO the lowland height\n",
    "# so that the maximum absolute elevation would be [lowland_elevation + max_mountain_elevation]\n",
    "# TODO should call this 'mountain_relief'???\n",
    "#############################################\n",
    "\n",
    "# the grid sampling for the output\n",
    "sampling = 0.5\n",
    "\n",
    "# this number controls how small polygons are exclude when merging the COB terranes into \n",
    "# land/sea masking polygons\n",
    "area_threshold = 0.0001\n",
    "\n",
    "# used for quadtree\n",
    "subdivision_depth = 2\n",
    "\n",
    "# this buffer defines the smoothness of the topography at the transition from 'lowland' to 'mountain'\n",
    "# the distance defined here is the distance over which heights ramp from the lowland elevation to the \n",
    "# mountain elevation defined above. (the ramping takes place from the edge of the mountain range inwards\n",
    "# towards the mountain interior). Any parts of the mountain range greater than this buffer distance from \n",
    "# the edge will have a uniform height equal to max_mountain_elevation\n",
    "mountain_buffer_distance_degrees = 1.\n",
    "#mountain_buffer_distance_degrees = 2.\n",
    "\n",
    "# choose here either 'ocean' or 'land'\n",
    "# this determines which grid takes precedence where both the age grid and the \n",
    "# paleogeographies overlap and contain valid values\n",
    "land_or_ocean_precedence = 'land'\n",
    "\n",
    "# this number is used in the final grdfilter step to smooth the output \n",
    "grid_smoothing_wavelength_kms = 400.\n",
    "\n",
    "\n",
    "####################################################\n",
    "\n",
    "# make a sorted list of the (midpoint) times for paleogeography polygons\n",
    "tmp = glob.glob('%s/*/' % reconstruction_basedir)\n",
    "\n",
    "time_list = []\n",
    "for tm in tmp:\n",
    "    time_list.append(float(re.findall(r'\\d+Ma+',tm)[1][:-2]))\n",
    "\n",
    "time_list.sort()\n",
    "\n",
    "time_list = np.array(time_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def write_xyz_file(output_filename, output_data):\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        for output_line in output_data:\n",
    "            output_file.write(' '.join(str(item) for item in output_line) + '\\n')\n",
    "\n",
    "\n",
    "# define a function that loads paleogeography multipoints at a specified time\n",
    "# NOTE this time can be anything, not a time where the multipoints fit nicely together,\n",
    "# hence the gaps and overlaps will be present\n",
    "def add_reconstructed_points_to_xyz(points_file,rotation_model,reconstruction_time,zval,mask_file=None):\n",
    "    \n",
    "    reconstructed_points = []\n",
    "    pygplates.reconstruct(points_file,rotation_model,reconstructed_points,reconstruction_time)\n",
    "    \n",
    "    mask_file = './OrogenMasks.gpml'\n",
    "    if mask_file is not None:\n",
    "        reconstructed_masks = []\n",
    "        pygplates.reconstruct(mask_file,rotation_model,reconstructed_masks,reconstruction_time)\n",
    "\n",
    "        for reconstructed_point in reconstructed_points:\n",
    "            for reconstructed_mask in reconstructed_masks:\n",
    "                if reconstructed_mask.get_reconstructed_geometry().is_point_in_polygon(reconstructed_point.get_reconstructed_geometry().get_centroid()):\n",
    "                    reconstructed_point.get_feature().set_name(reconstructed_mask.get_feature().get_name())\n",
    "        \n",
    "    point_array = []\n",
    "    zval_array = []\n",
    "    for reconstructed_point in reconstructed_points:\n",
    "        feature_coordinates = reconstructed_point.get_reconstructed_geometry().to_lat_lon_array()\n",
    "        point_array.append(feature_coordinates)\n",
    "        #if reconstructed_point.get_feature().get_feature_type() == pygplates.FeatureType.create_gpml('OrogenicBelt'):\n",
    "        if reconstructed_point.get_feature().get_name() == 'OrogenicBelt':\n",
    "            zval_array.append(np.ones((feature_coordinates.shape[0],1))*zval*3)\n",
    "        elif reconstructed_point.get_feature().get_name() == 'Cordillera':\n",
    "            zval_array.append(np.ones((feature_coordinates.shape[0],1))*zval*2)\n",
    "        else:\n",
    "            zval_array.append(np.ones((feature_coordinates.shape[0],1))*zval)\n",
    "            \n",
    "    xy_array = np.vstack(point_array)\n",
    "    xyz_array = np.hstack((xy_array,np.vstack(zval_array)))\n",
    "    \n",
    "    return xyz_array\n",
    "\n",
    "\n",
    "# function to facilitate the smoothing of topography\n",
    "# at the edge of mountain range polygons\n",
    "def get_distance_to_mountain_edge(point_array,reconstruction_basedir,time):\n",
    "    \n",
    "    distance_threshold_radians=None\n",
    "    env_list = ['m']\n",
    "\n",
    "    pg_dir = '%s/PresentDay_Paleogeog_Matthews2016_%dMa/' % (reconstruction_basedir,time)\n",
    "\n",
    "    pg_features = pg.load_paleogeography(pg_dir,env_list)\n",
    "    cf = pp.merge_polygons(pg_features,rotation_model,time=time,sampling=0.25)\n",
    "    sieve_polygons_t1 = pp.polygon_area_threshold(cf,area_threshold)\n",
    "\n",
    "    polygons_as_list = []\n",
    "    for feature in sieve_polygons_t1:\n",
    "        polygons_as_list.append(feature.get_geometry())\n",
    "        \n",
    "    res1 = find_closest_geometries_to_points([pygplates.PointOnSphere(point) for point in zip(point_array[:,0],point_array[:,1])],\n",
    "                                             polygons_as_list,\n",
    "                                             distance_threshold_radians = distance_threshold_radians)\n",
    "    \n",
    "    distance_to_polygon_boundary = np.degrees(np.array(zip(*res1)[0]))\n",
    "\n",
    "    # Make a copy of list of distances.\n",
    "    distance_to_polygon = list(distance_to_polygon_boundary)\n",
    "\n",
    "    # Set distance to zero for any points inside a polygon (leave other points unchanged).\n",
    "    res2 = points_in_polygons.find_polygons([pygplates.PointOnSphere(point) for point in zip(point_array[:,0],point_array[:,1])],\n",
    "                                            polygons_as_list)\n",
    "\n",
    "    for point_index, rpolygon in enumerate(res2):\n",
    "        # If not inside any polygons then result will be None.\n",
    "        if rpolygon is None:\n",
    "            distance_to_polygon[point_index] = 0.0\n",
    "            \n",
    "    return distance_to_polygon\n",
    "\n",
    "\n",
    "# This cell uses COB Terranes to make a masking polygon\n",
    "# (which is called 'seive_polygons')\n",
    "def get_merged_cob_terrane_polygons(COBterrane_file,reconstruction_time,sampling):\n",
    "\n",
    "    polygon_features = pygplates.FeatureCollection(COBterrane_file)\n",
    "\n",
    "    cobter = pp.force_polygon_geometries(polygon_features)\n",
    "\n",
    "    cf = pp.merge_polygons(cobter,rotation_model,time=reconstruction_time,sampling=sampling)\n",
    "    sieve_polygons = pp.polygon_area_threshold(cf,area_threshold)\n",
    "\n",
    "    return sieve_polygons\n",
    "\n",
    "\n",
    "\n",
    "# use merged seive_polygons to get a regular lat-long multipoint that will contain points\n",
    "# only within the COB Terranes (ie not within the 'deep ocean')\n",
    "def get_land_sea_multipoints(sieve_polygons,sampling):\n",
    "\n",
    "    multipoints = create_gpml_regular_long_lat_mesh(sampling)\n",
    "    grid_dims = (int(180/sampling)+1,int(360/sampling)+1)\n",
    "\n",
    "    for multipoint in multipoints:\n",
    "        for mp in multipoint.get_all_geometries():\n",
    "            points = mp.to_lat_lon_point_list()\n",
    "\n",
    "    #reconstructed_polygons = []\n",
    "    #pygplates.reconstruct(cobter,rotation_model,reconstructed_polygons,reconstruction_time)\n",
    "\n",
    "    rpolygons = []\n",
    "    for polygon in sieve_polygons:\n",
    "        if polygon.get_geometry():\n",
    "            rpolygons.append(polygon.get_geometry())\n",
    "\n",
    "    polygons_containing_points = points_in_polygons.find_polygons(points, rpolygons, subdivision_depth=subdivision_depth)\n",
    "\n",
    "    lat = []\n",
    "    lon = []\n",
    "    zval = []\n",
    "\n",
    "    lat_deep = []\n",
    "    lon_deep = []\n",
    "    zval_deep = []\n",
    "\n",
    "    for pcp,point in zip(polygons_containing_points,points):\n",
    "        if pcp is not None:\n",
    "            lat.append(point.get_latitude())\n",
    "            lon.append(point.get_longitude())\n",
    "        else:\n",
    "            lat_deep.append(point.get_latitude())\n",
    "            lon_deep.append(point.get_longitude())\n",
    "            zval_deep.append(depth_for_unknown_ocean)\n",
    "            \n",
    "    #plt.figure(figsize=(25,11))      \n",
    "    #plt.plot(lon,lat,'.')\n",
    "    \n",
    "    #plt.figure(figsize=(25,11))      \n",
    "    #plt.plot(lon_deep,lat_deep,'.')\n",
    "    \n",
    "    #plt.figure(figsize=(25,11))  \n",
    "    #for polygon in rpolygons:\n",
    "    #    plt.plot(polygon.to_lat_lon_array()[:,1],\n",
    "    #             polygon.to_lat_lon_array()[:,0])\n",
    "        \n",
    "            \n",
    "    return lat,lon,zval,lat_deep,lon_deep,zval_deep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Time 7.00Ma\n",
      "\n",
      "Selected Time is in the stage 6.00Ma to 14.00Ma\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_6Ma/m_fig64_11_2_PresentDay_Paleogeog_Matthews2016_6.00Ma.shp']\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_14Ma/m_fig62_20_11_PresentDay_Paleogeog_Matthews2016_14.00Ma.shp']\n",
      "Working on Time 8.00Ma\n",
      "\n",
      "Selected Time is in the stage 6.00Ma to 14.00Ma\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_6Ma/m_fig64_11_2_PresentDay_Paleogeog_Matthews2016_6.00Ma.shp']\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_14Ma/m_fig62_20_11_PresentDay_Paleogeog_Matthews2016_14.00Ma.shp']\n",
      "Working on Time 9.00Ma\n",
      "\n",
      "Selected Time is in the stage 6.00Ma to 14.00Ma\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_6Ma/m_fig64_11_2_PresentDay_Paleogeog_Matthews2016_6.00Ma.shp']\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_14Ma/m_fig62_20_11_PresentDay_Paleogeog_Matthews2016_14.00Ma.shp']\n",
      "Working on Time 10.00Ma\n",
      "\n",
      "Selected Time is in the stage 6.00Ma to 14.00Ma\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_6Ma/m_fig64_11_2_PresentDay_Paleogeog_Matthews2016_6.00Ma.shp']\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_14Ma/m_fig62_20_11_PresentDay_Paleogeog_Matthews2016_14.00Ma.shp']\n",
      "Working on Time 11.00Ma\n",
      "\n",
      "Selected Time is in the stage 6.00Ma to 14.00Ma\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_6Ma/m_fig64_11_2_PresentDay_Paleogeog_Matthews2016_6.00Ma.shp']\n",
      "['../paleogeography/Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_14Ma/m_fig62_20_11_PresentDay_Paleogeog_Matthews2016_14.00Ma.shp']"
     ]
    }
   ],
   "source": [
    "time_min = 7.\n",
    "time_max = 200.\n",
    "time_step = 1\n",
    "\n",
    "merge_with_bathymetry = False\n",
    "\n",
    "\n",
    "for reconstruction_time in np.arange(time_min,time_max+time_step,time_step):\n",
    "\n",
    "    print 'Working on Time %0.2fMa\\n' % reconstruction_time \n",
    "        \n",
    "    # find times that bracket the selected exact time in the paleogeography source files\n",
    "    time_stage_max = time_list[np.where(time_list>reconstruction_time)[0][0]]\n",
    "    time_stage_min = time_list[np.where(time_list<=reconstruction_time)[0][-1]]\n",
    "\n",
    "    # Note the logic for selecting the times:\n",
    "    # The main issue is that each set of paleogeography polygons has a defined 'midpoint' time\n",
    "    # --> if the reconstruction time is between these, the choice of t1 and t2 is obvious\n",
    "    # --> if the reconstruction time matches one of these times, then we can work directly on\n",
    "    #     the geometries that match this time - hence the two routes through the if statement below\n",
    "    \n",
    "    print 'Selected Time is in the stage %0.2fMa to %0.2fMa' % (time_stage_min,time_stage_max)\n",
    "\n",
    "    land_points_file = '%s/tweentest_land_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "    marine_points_file = '%s/tweentest_ocean_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "    mountains_going_up_file = '%s/mountain_transgression_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "    mountains_going_down_file = '%s/mountain_regression_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "    mountains_stable_file = '%s/mountain_stable_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "\n",
    "    \n",
    "    # get a nx3 array defining the points above sea-level, reconstructed to time of interest\n",
    "    # columns are [lat, long, elevation assigned for lowland]\n",
    "    land_point_array = add_reconstructed_points_to_xyz(land_points_file,\n",
    "                                                       rotation_model,\n",
    "                                                       reconstruction_time,\n",
    "                                                       lowland_elevation)\n",
    "    \n",
    "    # get a nx3 array defining shallow marine areas, reconstructed to time of interest\n",
    "    # columns are [lat, long, elevation assigned for shallow marine]\n",
    "    marine_point_array = add_reconstructed_points_to_xyz(marine_points_file,\n",
    "                                                         rotation_model,\n",
    "                                                         reconstruction_time,\n",
    "                                                         shallow_marine_elevation)\n",
    "    \n",
    "    # Note that the two arrays just created are based on 'regular' lat/long grids, but \n",
    "    # are not aligned with the regular lat/long grid that we want to output\n",
    "    # since they are (usually) reconstructed to a different time from the one at which they\n",
    "    # were created (and anyway may be at a different resolution to the grid sampling specified\n",
    "    # here)\n",
    "\n",
    "    # combine the previous two arrays\n",
    "    pg_point_array = np.vstack((land_point_array,marine_point_array))\n",
    "\n",
    "    # get a merged version of COB terranes, optionally excluding polygons that are small in area\n",
    "    # TODO deal with donut polygons better\n",
    "    sieve_polygons = get_merged_cob_terrane_polygons(COBterrane_file,reconstruction_time,sampling)\n",
    "\n",
    "    # get arrays defining the land and sea based on which points fall within the COB terranes\n",
    "    # NOTE this step is where we create the points that ARE on the regular lat/long grid we \n",
    "    # will ultimately output\n",
    "    (lat,lon,zval,\n",
    "     lat_deep,lon_deep,zval_deep) = get_land_sea_multipoints(sieve_polygons,sampling)\n",
    "\n",
    "\n",
    "    # sample the land/marine points onto the points within the COB Terranes\n",
    "    # This will fill the gaps that exist within continents, and average out overlaps\n",
    "    d,l = sampleOnSphere(pg_point_array[:,0],pg_point_array[:,1],pg_point_array[:,2],\n",
    "                         np.array(lat),np.array(lon),n=1)\n",
    "\n",
    "    land_marine_interp_points = pg_point_array[:,2].ravel()[l]\n",
    "\n",
    "    # At this point, the land points are all considered to be 'lowland'......\n",
    "    \n",
    "    ####################################\n",
    "    # Deal with the mountains\n",
    "    if np.equal(reconstruction_time,time_stage_min):\n",
    "        print 'Temporary hack for valid time'\n",
    "        #dat3 = add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,reconstruction_time,3)\n",
    "        dat4 = add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,reconstruction_time+0.01,1)\n",
    "        dat5 = add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,reconstruction_time+0.01,1)\n",
    "        mountains_tr_point_array = np.vstack((dat4,dat5))\n",
    "        \n",
    "        dist_tr = get_distance_to_mountain_edge(mountains_tr_point_array,reconstruction_basedir,reconstruction_time)\n",
    "        dist_tr_cap = np.array(dist_tr)\n",
    "        dist_tr_cap[np.array(dist_tr)>mountain_buffer_distance_degrees] = mountain_buffer_distance_degrees\n",
    "\n",
    "        normalized_mountain_elevation = dist_tr_cap\n",
    "        \n",
    "    else:\n",
    "        # load in the mountain points but at three different times: t1 and t2, and the reconstruction time\n",
    "        # note that these three arrays should all be identical in size, since they are the same multipoints\n",
    "        # just reconstructed to three slightly different times\n",
    "        dat3 = add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,time_stage_max,1)\n",
    "        dat4 = add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,time_stage_max,1)\n",
    "        dat5 = add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,time_stage_max,1)\n",
    "        mountains_t2_point_array = np.vstack((dat3,dat4,dat5))\n",
    "\n",
    "        dat3 = add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,time_stage_min+0.01,1)\n",
    "        dat4 = add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,time_stage_min+0.01,1)\n",
    "        dat5 = add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,time_stage_min+0.01,1)\n",
    "        mountains_t1_point_array = np.vstack((dat3,dat4,dat5))\n",
    "\n",
    "        dat3 = add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,reconstruction_time,1)\n",
    "        dat4 = add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,reconstruction_time,1)\n",
    "        dat5 = add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,reconstruction_time,1)\n",
    "        mountains_tr_point_array = np.vstack((dat3,dat4,dat5))\n",
    "\n",
    "        # calculate distances of the mountain points to the edge of the mountain region at t1 and t2,\n",
    "        # using the pg polygons that they should exactly correspond to \n",
    "        dist_t1 = get_distance_to_mountain_edge(mountains_t1_point_array,reconstruction_basedir,time_stage_min)\n",
    "        dist_t2 = get_distance_to_mountain_edge(mountains_t2_point_array,reconstruction_basedir,time_stage_max)\n",
    "        \n",
    "        #is_in_orogeny_index = find_mountain_type(mountains_tr_point_array,\n",
    "        #                                         orogeny_feature_filename,\n",
    "        #                                         reconstruction_time)\n",
    "\n",
    "\n",
    "        # cap the distances at some arbitrary value defined earlier\n",
    "        dist_t1_cap = np.array(dist_t1)\n",
    "        dist_t1_cap[np.array(dist_t1)>mountain_buffer_distance_degrees] = mountain_buffer_distance_degrees\n",
    "\n",
    "        dist_t1_cap = dist_t1_cap*mountains_t1_point_array[:,2]\n",
    "        \n",
    "        dist_t2_cap = np.array(dist_t2)\n",
    "        dist_t2_cap[np.array(dist_t2)>mountain_buffer_distance_degrees] = mountain_buffer_distance_degrees\n",
    "        \n",
    "        dist_t2_cap = dist_t2_cap*mountains_t2_point_array[:,2]\n",
    "\n",
    "        # get the normalised time within this time stage\n",
    "        # for example we are at 0.25 between the t1 and t2\n",
    "        t_diff = (time_stage_max-time_stage_min)\n",
    "        t_norm = (reconstruction_time-time_stage_min)/t_diff\n",
    "        #print t_diff, t_norm\n",
    "\n",
    "        # use 1d interpolation to get the 'normalized' height of the mountains at the preceding\n",
    "        # and subsequent times to the specific reconstruction time\n",
    "        # [note this is not spatial interpolation - rather it is interpolation at each individual point\n",
    "        # between the heights at earlier and later times]\n",
    "        tmp = np.vstack((dist_t1_cap,dist_t2_cap))\n",
    "        f = interpolate.interp1d([0,1],tmp.T)\n",
    "        normalized_mountain_elevation = f(t_norm)\n",
    "    \n",
    "    \n",
    "    #plt.figure(figsize=(25,11))\n",
    "    #plt.plot(mountains_tr_point_array[:,1],mountains_tr_point_array[:,0],'.')\n",
    "    \n",
    "    # interpolate the elevations at tr onto the regular long lat points that we will ultimately use \n",
    "    # for the grid output\n",
    "    # note the k value here controls number of neighbouring points used in inverse distance average\n",
    "    d,l = sampleOnSphere(mountains_tr_point_array[:,0],mountains_tr_point_array[:,1],normalized_mountain_elevation,\n",
    "                         np.array(lat),np.array(lon),k=4)\n",
    "    w = 1./d**2\n",
    "    normalized_mountain_elevation_interp_points = np.sum(w * normalized_mountain_elevation.ravel()[l],axis=1) / np.sum(w,axis=1)\n",
    "\n",
    "    # this index isolates only those points that are within a certain distance of the mountain range\n",
    "    # (since the interpolation will give values everywhere in the 'land', so we want to re-isolate only \n",
    "    # those points that fall within the 'mountain' regions)\n",
    "    # TODO this should be set to the sampling??\n",
    "    mountain_proximity_index = np.degrees(np.min(d,axis=1))< sampling*2 #mountain_buffer_distance_degrees\n",
    "\n",
    "    plotting=True\n",
    "    if plotting:\n",
    "        plt.figure(figsize=(25,11))\n",
    "        plt.scatter(mountains_tr_point_array[:,1],mountains_tr_point_array[:,0],\n",
    "                    c=normalized_mountain_elevation,\n",
    "                    edgecolor='',s=2)\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.figure(figsize=(25,11))\n",
    "        plt.scatter(np.array(lon)[mountain_proximity_index],\n",
    "                    np.array(lat)[mountain_proximity_index],\n",
    "                    c=normalized_mountain_elevation_interp_points[mountain_proximity_index],\n",
    "                    edgecolor='',s=2)\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.figure(figsize=(25,11))\n",
    "        plt.scatter(mountains_tr_point_array[:,1],mountains_tr_point_array[:,0],\n",
    "                    c=mountains_t1_point_array[:,2],\n",
    "                    edgecolor='',s=4)\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.figure(figsize=(25,11))\n",
    "        plt.scatter(np.array(lon),\n",
    "                    np.array(lat),\n",
    "                    c=normalized_mountain_elevation_interp_points,\n",
    "                    edgecolor='',s=2)\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.figure(figsize=(25,11))\n",
    "        plt.scatter(np.array(lon),\n",
    "                    np.array(lat),\n",
    "                    c=np.min(d,axis=1),\n",
    "                    edgecolor='',s=2)\n",
    "        plt.colorbar()\n",
    "\n",
    "    \n",
    "    #####################################\n",
    "    # Put the grid together\n",
    "    #####################################\n",
    "    \n",
    "    # write the land/marine points to a file\n",
    "    write_xyz_file('tmp/land_marine.xyz',zip(lon+lon_deep,\n",
    "                                         lat+lat_deep,\n",
    "                                         np.hstack((land_marine_interp_points,zval_deep))))\n",
    "\n",
    "    # convert the normalized mountain elevations to metres, then write to file\n",
    "    mountain_elevation_factor = max_mountain_elevation/mountain_buffer_distance_degrees\n",
    "    mountain_elevation_array = normalized_mountain_elevation_interp_points[mountain_proximity_index]*mountain_elevation_factor\n",
    "    write_xyz_file('tmp/mountain.xyz',zip(np.array(lon)[mountain_proximity_index],\n",
    "                                      np.array(lat)[mountain_proximity_index],\n",
    "                                      mountain_elevation_array))\n",
    "\n",
    "    # all the points are already on the same regular lat/long grid (but with gaps) - just \n",
    "    # need to piece them all together and combine.\n",
    "    # Note we assume the the mountain elevation is the height IN ADDITION to the lowland elevation\n",
    "    # so that we can simply add them\n",
    "    os.system('gmt xyz2grd tmp/land_marine.xyz -Gtmp/land_marine.nc -Rd -I%0.8f' % sampling)\n",
    "    os.system('gmt xyz2grd tmp/mountain.xyz -Gtmp/mountain.nc -Rd -I%0.8f -di0' % sampling)\n",
    "    os.system('gmt grdmath tmp/mountain.nc tmp/land_marine.nc ADD = %s/paleotopo_%0.2fMa.nc' % (output_dir,reconstruction_time))\n",
    "\n",
    "    # load result back into python\n",
    "    topoX,topoY,topoZ = pg.load_netcdf('%s/paleotopo_%0.2fMa.nc' % (output_dir,reconstruction_time))\n",
    "\n",
    "    \n",
    "    if merge_with_bathymetry:\n",
    "    \n",
    "        # PALEOBATHYMETRY based on age grids\n",
    "        # load age grid for this time and calculate paleobathymetry\n",
    "        agegrid_file = agegrid_file_template % reconstruction_time\n",
    "\n",
    "        ageX,ageY,ageZ = pg.load_netcdf(agegrid_file)\n",
    "\n",
    "        paleodepth = pg.age2depth(ageZ,model='GDH1')\n",
    "\n",
    "\n",
    "        # get index for grid nodes where age grid is nan, replace values with topography/shallow bathymetry\n",
    "        not_bathy_index = np.isnan(paleodepth)\n",
    "        paleodepth[not_bathy_index] = topoZ[not_bathy_index]\n",
    "\n",
    "\n",
    "        # save the merged grid (forcing compatibility with GPlates-readable netCDF in case it helps)\n",
    "        ds = xr.DataArray(paleodepth,\n",
    "                           coords=[('lat',topoY),('lon',topoX)])\n",
    "        ds.to_netcdf('tmp/paleotopobathy.nc',format='NETCDF3_CLASSIC')\n",
    "\n",
    "        # smooth the grid using GMT [wavelength is optional\n",
    "        #pg.smooth_topography_grid('paleotopobathy.nc','paleotopobathy_smooth_%0.2fMa.nc' % reconstruction_time,400.)\n",
    "        os.system('gmt grdfilter %s -G%s -Fg%0.2f -fg -D4 -Vl' % ('tmp/paleotopobathy.nc',\n",
    "                                                                 'tmp/paleotopobathy_smooth.nc',\n",
    "                                                                 grid_smoothing_wavelength_kms))\n",
    "\n",
    "        # finally, once again force GPlates-readable netCDF (ie netCDF v3) and put the \n",
    "        # grid in the output folder with a filename containing the age\n",
    "        os.system('gmt grdconvert %s -G%s=cf' % ('tmp/paleotopobathy_smooth.nc',\n",
    "                                                 '%s/paleotopobathy_smooth_%0.2fMa.nc' % (output_dir,reconstruction_time)))\n",
    "\n",
    "        # load and plot the result\n",
    "        topo_smoothX,topo_smoothY,topo_smoothZ = pg.load_netcdf('tmp/paleotopobathy_smooth.nc')\n",
    "        #\n",
    "        plt.figure(figsize=(25,11))\n",
    "        plt.imshow(topo_smoothZ,origin='lower',\n",
    "                   extent=[-180,180,-90,90],cmap=plt.cm.terrain)\n",
    "        plt.title('%0.2fMa' % reconstruction_time)\n",
    "        plt.colorbar()\n",
    "\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
